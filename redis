# Redis知识点

## Redis基础

### Redis整体架构

缺个图，网上没找到，只能自己画了 ->__->



#### Redis是什么？要解决什么问题？

redis是一个基于内存的分布式的KV数据存储。

redis主要是为了解决数据的快速存取。



#### 架构模型

网络IO层、执行层、存储层、集群层



### Redis 数据结构

学习路径：有哪些数据结构？实现原理是什么？不同数据结构解决的问题是什么？不同数据结构有哪些问题？

#### Redis为什么要设计多种数据结构？

解决不同场景的查询和使用需求；比如排序、队列、去重、地图信息等功能



#### Redis 底层数据结构

SDS（简单动态字符串）、双向链表、哈希表、跳表、压缩链表、整数数组



#### Redis 操作的数据类型

String、List、Hash、Set、Sorted Set、BitMap、GEO



#### 底层数据结构分析

##### SDS

###### 原理

自定义的数据结构，内存结构：len(已使用的长度)、free(未使用的长度)、buf(字符数组)

###### 使用场景（解决问题）

解决了C语言字符串的问题

**安全性：**

1. 杜绝了缓冲区溢出问题：C语言字符串拼接（修改）时可能会造成的缓冲区溢出（strCat）
2. 支持二进制数据：C语言字符串无法支持二进制数据（遇到空字符就判定字符串结束）

**效率：**

1. 减少了字符串更新（新增、删除）造成的内存重新分配次数，内部预置了额外的内存空间，并使用惰性删除
2. O(1)的复杂度获取字符串的长度

###### 存在什么问题

1. 惰性删除和内存预分配，存在一定的空间浪费
2. Big Key的数据如何处理？是否会有影响？



##### 双向链表

###### 原理

简单的双向链表，头尾节点，前后节点



##### 字典

###### 原理

实现为哈希表

**结构：**dict[2]（两个哈希表，写时复制原理），rehashIdx（渐进式rehash时使用，标识当前rehash进度；非rehash时值为-1）

**哈希算法：**hash（Value）& (size-1)，需要保证size大小为2的n次幂

**哈希冲突：**使用拉链法来解决冲突，数据写入时使用头插法（O(1)复杂度）

**扩容与缩容：**

1. 条件：同时参考 **负载因子**与**后台线程**（BGSAVE、BGREWRITEAOF）
2. 过程：依据写时复制原理，根据rehashIdx位置逐渐将dict[0]中的数据迁移至dict[1]中；

扩容期间保证读写：

1. 读：先读dict[0]、不存在则再读dict[1]
2. 写：dict[0]中不存在则直接写入dict[1]，如果dict[0]存在则更新或者删除后再写入dict[1]



##### 跳表

**为什么使用跳表不使用红黑树？**



###### 原理

使用类二分法的原理，实现了链表的二分查找；

使用多个**层**来记录数据，不同的层，就是不同的二分过程数据；

层之间的跨度用来记录两个节点之间的距离，作为计算RANK的依据；



##### 整数集合（intset）

如果一个set集合中全是整数，那么就会使用intset来保存，进而节约内存

###### 原理

内部使用一个整数数组，该数组使用有序、无重复的方式来保存元素；

**查询：**由于有序，可以使用二分法查询，复杂度lgn

**新增：**由于新增涉及排序和元素位置调整，复杂度为O(N)



##### 压缩列表

###### 原理

是一组连续内存空间的顺序型数据结构；zltail（队尾元素的首地址位置）、zllen、entry（各个元素数据）

后置节点会保存前一个节点的字节大小，然后根据指针计算出前一个元素的首地址；

###### 解决的问题

主要是为了节省空间，每个节点少了指向前后元素的指针（因为如果元素过小时，可能导致指针占用的空间比实际数据占用的空间还大）；

###### 存在的问题

**查询：**与链表查询复杂度一致，都是O(N) ，需要遍历所有元素

**写入：**由于是连续空间的数据，且后置节点保留了前置节点的大小，所以队首元素写入可能造成后续所有元素的连锁更新；删除也是同理



##### GEO



##### BitMap

redis内部通过char数组（一个char为8bit）来实现BitMap

提供了SetBit（设置某一位的值）、GetBit（获取某一位的值）、BitCount（统计值1的个数）、BitTop（不同BitMap之间做位运算） 命令

###### 如何实现BitCount（统计值为1的个数）



###### 优缺点

**优点：**

1. 占用内存空间较小
2. 可以精确统计

**缺点：**

1. 当数据量大的时候，一次性分配内存过大会阻塞redis
2. BITTOP复杂度为O(N)，在主节点执行会造成redis阻塞（建议在低峰期在从节点执行）



##### HyperLogLog

基于概率统计的维度进行数据统计，可以用很少的内存（14K）就能统计很大的数据量（2^64的数据）

**使用场景：**对于数据要求不精确的统计数据

###### 操作

PFADD 添加一个元素，如果重复，只算作一个
PFCOUNT 返回元素数量的近似值
PFMERGE 将多个 HyperLogLog 合并为一个 HyperLogLog



### Redis IO模型

#### IO模型

https://segmentfault.com/a/1190000003063859

IO过程主要分为两步（以读取为例）：

1. 等待数据就绪（网络数据包或磁盘数据），即等待数据从网络或磁盘拷贝到内核空间中
2. 将数据从内核空间拷贝到用户空间

##### 阻塞IO（Blocking IO）

![preview](https://segmentfault.com/img/bVm1c3/view)

当用户进程发起read操作时，会进行内核操作，如果内核数据还没有准备就绪，则会将用户线程挂起，用户线程就处于阻塞状态等待后续内核的唤醒；

内核数据就绪，并将数据拷贝到用户空间后，内核唤醒用户线程，阻塞结束；

**由于整个IO操作过程中，用户线程一直处理阻塞状态，所以称为阻塞IO。**



##### 非阻塞IO（nonBlocking IO）

![preview](https://segmentfault.com/img/bVm1c4/view)

当用户进程发起read操作时，内核数据如果没有准备就绪，就直接返回一个错误给用户进程，而不会将用户进程挂起；

用户进程不断轮询内核，等内核数据准备就绪后，内核就会将用户进程挂起，等待内核将数据拷贝到用户空间后唤醒用户线程；

**在整个IO操作中，第一步等待内核数据就绪的过程是不会阻塞用户线程的，所以被称为非阻塞IO。**



##### IO多路复用（IO multiplexing）

![preview](https://segmentfault.com/img/bVm1c5/view)

一个用户进程可以同时处理多个网络IO；

当用户进程进行select() 的系统调用后，会阻塞直到有符合的非阻塞的FD（文件描述符）就绪时；

Select()调用返回后，会获取所有就绪的FD，然后再进行系统调用，等待内核将数据拷贝到用户空间；

IO多路复用使用的系统函数是select()、poll()、epoll()；



##### 信号驱动IO（signal driven IO）

当用户向内核发送read请求时，内核会直接返回，等到内核数据就绪会，再返回一个信号或中断给到用户进程；

然后用户进程向内核发生系统调用，等待内核将数据拷贝到用户空间；（这步是会block用户进程）



##### 异步IO（asynchronous IO）

用户向内核发送aio_read请求后，内核直接返回一个结果给用户进程。

等内核数据就绪，并且内核将数据拷贝到用户空间后，内核向用户进程发送一个信号；

**在整个IO操作中，所有内核的操作都不会阻塞用户进程，所以被称为异步IO。**



##### 阻塞IO与非阻塞IO的区别

阻塞IO调用时，如果内核数据还未就绪会一直block；

非阻塞IO调用时，如果内核数据还未就绪会直接返回，不用等待；



##### 同步IO与异步IO区别

同步IO是指，整个IO操作中会存在用户进程的阻塞；（包括内核数据的就绪、数据从内核空间拷贝到用户空间）

异步IO是指，整个IO操作都不会阻塞用户进程；

Blocking IO、IO多路复用、NonBlocking IO都是同步IO；

只有AIO才是真正的异步IO；



##### select()、poll()、epoll()的区别

Select() 和 poll() 类似，都是通过对内核进行**轮询**来获取就绪的FD；但是这个过程中，内核会扫描所有连接的FD，并将所有的FD从内核空间拷贝到用户空间，随后在用户空间遍历所有FD，过滤掉未就绪的FD；但是select()有数量限制，单个进程只能监听1024个FD；但是poll()没有这个限制，通过链表实现；

从本质来说，select()与poll() 返回后，都是通过**遍历所有FD来获得已就绪的FD**，随着连接数量过多，效率会线性下降；



epoll() 是会向内核注册一个FD，并在这个FD上注册关心的事件，以及收到关心事件后的**回调函数**，通过这个注册FD来管理所有的FD；

一旦某个FD就绪，内核会激活注册的FD，用户进程调用epoll_wait() 就会收到事件通知；



select()与poll()的特点是：1. 用户进程主动轮询 2. 每次会扫描所有的FD；

epoll() 的特点：1. 没有FD数量的限制；2. 内核会通过回调的方式将已就绪的FD返回，而无需用户线程去轮询获取已就绪的FD；





#### Reactor模式（redis采用的IO模式）

http://www.blogjava.net/DLevin/archive/2015/09/02/427045.html

**定义：**The reactor design pattern is an **event handling pattern** for handling service requests delivered concurrently by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to associated request handlers

**特点：**

1. 基于select/epoll 的IO多路复用；
2. 事件驱动的模式；
3. 类似生产者-消费组模式，Service Handler基于epoll()的方式会监听就绪的IO事件（同一时间会有多个FD就绪），然后再**同步**的将事件分发给对应的事件处理器；
4. IO读写与事件处理在同一个线程中处理（单线程处理）；





### Redis持久化

因为redis的数据全部都是保存在内存中，机器重启后数据就会丢失，redis的持久化就是为了故障重启后数据的快速恢复；

#### AOF

##### 是什么？

redis会将每个**执行完成**的**写命令**追加到一个AOF文件中（写后日志），在redis故障重启时，会读入aof文件来进行**数据的恢复**；

##### AOF写入过程

1. 命令追加：命名执行完成后，将命令写入**AOF缓冲区**中；
2. 文件写入：将AOF缓冲区的数据写入文件；（只是写入操作系统的文件缓冲区中，并没有真正的写入到磁盘）
3. 文件同步：将文件缓冲区的数据刷入磁盘；



##### AOF持久化策略

**ALWAYS:**  每次命令成功会同步将数据写入磁盘；（会同步将数据刷入磁盘）

**Everysec：**每秒回写，先把命令写入AOF内存缓冲区，每秒钟进行数据的刷盘；

**NO：**把命令写入AOF缓冲区中，然后由操作系统决定什么时机进行刷盘；

三种策略都是在性能与可靠性之间的权衡；



##### AOF重写

由于AOF文件会一直追加，会导致文件过大，文件过大会导致：1. 操作系统有文件大小限制，不能无限大；2. 文件过大，写入效率会变低；3. 故障恢复时，需要执行AOF文件中的所有命令，文件过大故障恢复时间过长；

对某一个key的多次操作会被写入多条，但实际key只会有一个结果，所以可以将某个key的多次操作合并为一条记录，这个过程就是AOF重写。

AOF重写是基于**内存中的数据**来进行重写，不需要读取现有的AOF文件。

###### AOF重写过程

1. 主进程fork() 一个后台子进程（BGREWRITEOF），子进程拥有父进程的所有数据（fork那刻的内存快照数据）
2. 子进程读取内存中的数据，并进行AOF重写
3. 子进程进程AOF重写期间，主进程收到命令会将数据写入**AOF缓冲区**和**AOF重写缓冲区**（保证已有的AOF文件可用，同时保证重写的文件数据不会与已有文件不一致）
4. 子进程处理完成后，通知主进程，由主进程接着进行AOF重写（主进程读取AOF重写缓冲区中的数据，并将数据写入新的AOF文件），主进程的重写过程会阻塞新的请求（单线程处理，没办法~~）；



**关键点：**

如何保证AOF重写期间，主进程处理的数据可以正确地保存到AOF重写文件中；



#### RDB

##### 是什么？

保存了redis服务器内存中所有数据的二进制文件；

##### 解决什么问题？与AOF区别？

与AOF一样都是为了服务重启后数据的快速恢复；

AOF文件恢复速度慢（特别是AOF文件过大时），RDB的恢复速度快；

##### 生成RDB的过程

1. 主进程fork() 一个子进程（BGSAVE），与AOF重写类似
2. 子进程进行RDB文件的生成
3. 由于是某个固定时间的内存快照，不需要保证数据与主进程实时一致



##### 生成RDB的频率（触发RDB生成的条件）

1. 时间 + 变更操作的数量（可配置）：比如每60秒 1000次修改 触发RDB生成



##### 使用策略

redis4.0中：定期RDB + 增量AOF的方式保证数据的完整性



#### AOF与RDB对比

1. AOF重写通过aof缓冲区的方式，保证了操作的不中断与数据的一致性；RDB是某一刻的内存快照
2. AOF是增量的命令写入，RDB是全量的内存快照
3. AOF的恢复速度比RDB的恢复速度慢
4. AOF占用的空间比RDB空间大



#### AOF与RDB使用的场景



### Redis 数据淘汰

#### 淘汰策略

redis有8个淘汰策略

**所有key不淘汰策略**：noeviction

**针对所有key的策略**：allKyes-LRU（最近最少使用）、allKeys-LFU（最近不经常使用）、allkeys-random

**针对设置过期时间的key**：volatile-LRU、volatile-random、volatile-ttl、volatile-LFU



#### Redis的LRU实现

##### 通常实现

通常LRU的实现是一个双向链表，最近一次访问的数据移到队首，时间长未访问的数据靠近队尾；

可以使用Java中的LinkedHashMap来实现；O(1)的查找和移动；

##### redis实现

由于通常的实现需要维护一个链表，redis可能有上千万的Key，维护链表的成本太高；

**权衡实现：**redis随机抽取N个数据作为候选数据集，等下次淘汰时，再挑选N个数据与候选集合中的数据集进行比较，只有时间比候选集合中的数据早的数据才会被添加到集合中被淘汰。



#### LRU与LFU的缺点

1. LFU对于短时间内的高频访问会长久的留驻内存，导致新的数据被淘汰
2. LRU对于访问频率比较高周期性的数据不友好，会被频繁淘汰；





## Redis 集群

### 主从同步

#### 解决的问题？如何使用？

解决单点故障服务不可用的问题；

从库只读，主库读写；



#### 全量同步

##### 触发条件：

1. 主节点和从节点第一次建立连接  
2. 从节点断开连接时间过长，从节点的偏移量小于主节点**复制积压缓冲区**（replication backlog buffer） 中的数据；

##### 全量同步过程：

1. 从节点发起全量同步的命令；
2. 主节点生成RDB文件，同时创建一个**复制缓冲区**（replication buffer，每个从库都有一个），将增加的命令写入复制缓冲区中；
3. 主节点将RDB文件传输给从节点，从节点清空本地数据，加载RDB数据；
4. 从节点加载完RDB文件后，再获取主节点复制缓冲区中的命令；
5. 读取完复制缓冲区数据后，主从节点保持数据一致；



#### 增量同步

主从节点全量同步后，便通过**命令传播**的方式进行增量同步；

如果发生从节点与主节点之间网络的中断，就出现主从节点的不一致，增量同步就是解决这个问题；

##### 增量同步过程：

1. 主节点在进行命令传播时，会同时将命令放入本地的**复制积压缓冲区**(replication backlog buffer)中，且复制积压缓冲区中的每个数据都会有一个偏移量值（offset）；（复制积压缓冲区是一个**固定大小的队列**，所有从节点都对应这一个缓冲区，当队列满了之后会将队首的数据删除，然后再将新的数据写入队列尾部）
2. 从节点在进行数据同步时，会记录自己的偏移量（offset值）；主从数据一致时，偏移量就是队尾数据的偏移量
3. 从节点与主节点断开重连后，从节点向主节点发送增量同步命令，携带从节点当前复制的offset（复制偏移量）；
4. 如果主从数据不一致的话，且offset还在主节点的复制积压缓冲区中，则主节点返回一个continue命令，并将offset至最新的数据都发送给从节点（同时主节点会将最新的offset值传给从节点），从节点收到数据后进行数据同步，主从数据一致；
5. 如果offset值不在复制积压缓冲区中，则主节点通知从节点无法进行增量同步，只能重新进行全量同步；



#### 总结：

1. redis解决增量数据复制时（AOF重写、全量复制、增量同步），都是通过使用缓冲区来解决的；



### 哨兵机制（sentinel）

#### 是什么？解决什么问题？

哨兵是一个特殊的redis进程，主要是负责主从切换保证redis服务的稳定性；

哨兵主要的工作分为：监控、选主、故障转移（主从切换）

#### 哨兵初始化

哨兵启动后，通过用户配置，查找需要监听的**主节点信息**；（一个哨兵可以监听多个主节点）

哨兵定时给主节点发送INFO信息来获取当前主节点对应的从节点信息；

#### 哨兵集群

哨兵之间通过redis的pub/sub的方式来进行互相发现，形成哨兵集群；

#### 主观下线

哨兵定时给主从节点发送PING信息（类似心跳包），如果redis节点没有在**down-after-milliseconds**（用户配置的时间，不同哨兵节点可以配置的不同，但是建议配置为相同的） 时间内返回有效回复（PONG、LOADING、MASTERDOWN），则redis节点被当前哨兵判定为主观下线

#### 客观下线

当某个哨兵判定redis主节点为主观下线后，将发送消息给其它哨兵节点，如果超过半数（数量可配置）哨兵节点都认为主节点为主观下线，则主节点就被标记为客观下线

#### 哨兵选举

哨兵对主节点标记完客观下线后，需要进行故障转移，再进行故障转移前需要进行**哨兵选举**（RAFT选举算法），选举出一个**领头哨兵**；

然后通过领头哨兵来对故障节点进行下线和主节点的上线；

#### 哨兵选主

领头哨兵需要在多个从节点中筛选出一个符合条件的节点来作为主节点，筛选过程如下（主要是为了筛选出**包含最新数据的从节点**）：

1. 去除所有已判断下线的节点；
2. 排除主从节点之间断开时间超过**down-after-milliseconds*10** 的从节点；（保留有最新数据的从节点，断开时间过长说明数据越旧）
3. 再从剩余从节点中选出复制偏移量（offset）最大的节点；

#### 故障转移（主从切换）

领头哨兵选择主节点后，将通知集群中其它的从节点，让剩余的从节点对新主节点执行SLAVEOF 命令，成为新主节点的从节点，并进行数据同步；

将已下线的主节点标记为新主节点的从节点，等已下线主节点上线后再同步新主节点的数据；



### 切片集群

单机redis 的容量有限，扩展较为困难，所以通过分布式的方式，将数据分散到不同节点上，增加了redis存储数据的能力。

#### 槽指派

**单机的槽指派**：redis在集群模式下运行时，需要将0~65535之间的槽分配给集群内的节点，如果槽未分配完，集群无法运行；

**节点间数据发现**：节点间通过Gossip协议（两两之间互相握手）进行节点间数据的发现与交换；

#### 节点数据结构

1. 存储当前节点有哪些槽（可以快速查找当前节点是否负责某个槽） ： Char slots[16384/8] ，类似BitMap的形式存储
2. 存储每个槽对应的负责节点信息（可以快速查找槽是哪个节点负责）：clusterNode *slots[16384]

#### 集群间数据读写

1. 客户端向节点发送命令，节点根据key计算出槽位置，查看对应的槽属于哪个节点负责；
2. 如果对应槽由当前节点负责，则当前节点执行命令；
3. 如果对应槽由当前其它节点负责，则给客户端返回一个 MOVED错误，并返回对应处理节点的ip：port
4. 客户端重定向到新节点，发送新的命令

#### 重新分片

集群间的节点可能会发生增减，这时就涉及到槽的重新分配；（如何保证槽分配期间集群的可用性）

**ASK命令实现迁移过程中集群的可用**：槽迁移过程中，导出节点和导入节点会存储正在导出和导入的槽；

客户端请求一个key时，如果这个key对应的槽正在被迁移，且对应的key已经被迁移完，这时当前节点就会返回一个ASK错误给客户端，客户端收到ASK命令后会请求新节点执行命令；

#### 故障检测

**主观下线与客观下线**

集群中各个主节点就类似哨兵的功能，主节点之间通过Gossip协议不断发送PING；如果某个节点没有返回有效信息，则被标识为疑似下线，如果集群中超半数节点认为主节点下线，则主节点被标记为下线；

#### 故障转移

1. 某个主节点被标记为下线后，剩余主节点通过Raft协议，选择一个领头主节点；
2. 领头主节点从下线主节点的从节点中选择一个从节点提升为主节点；



#### 相关问题：

##### redis cluster为什么使用16384个槽？

作者回答：https://github.com/redis/redis/issues/2576

网上类似的解答：https://zhuanlan.zhihu.com/p/99037321

**总结如下：**

1. **槽多会导致节点间通信数据大，占用网络带宽**：redis节点之间PING 通信时，会通过BitMap的形式将当前节点的槽信息发送出去；CRC16计算的hash值是16位的，数据范围是0~65535，如果槽数量为65535则会使用8KB（redis采用char来存数据，一个char是8bit，计算：65535 / 8 / 1024）的数据，而16384则使用2KB的数据，相对来说较小；
2. redis集群规模有限，一般不超过1000，所以16384个槽足够了
3. bitMap的压缩率相关，如果节点越少，槽越少，则压缩率高；



##### redis的数据分片与MySQL的数据分片？

多大的数据适合分片？分片的依据是什么？节点的增减如何处理？





# Redis实践

### 数据结构使用

#### 集合的使用

Set的差集（SDIFFSTORE）、并集（SINTERSTORE）、交集（SUNIONSTORE）操作

##### 场景一：计算每天的新增用户

Set1（user:date，userSet）：保存每天登陆的用户（假设注册后的用户一定会触发登陆操作）

Set2（user:total, userSet）：保存所有登陆过的用户（前一天的）

新增用户集合：SDIFFSTORE user:new user:date user:total

更新Set2的集合保证为全部用户：SUNIONSTORE user:total user:total user:date

##### 场景二：计算第二天留存的用户（连续两天都登陆的用户）

SINTERSTORE user:stay user:20210101 user:20210102



#### 排序使用

Sorted Set 用户根据分数实现排序；

List 队列的方式可以在时间维度上保证顺序；

##### 场景：排行榜使用



#### 二值状态统计、基数统计

使用BitMap来统计，bitMap本身每一位的值只有0和1

##### 场景：统计连续10天签到的用户数

单日数据：将所有用户每天签到的情况存入一个bitMap中

连续十日签到：对十天的bitMap数据做“与”操作

##### 场景：日活统计、月活统计、UV统计

日活：每天存一个HLL

月活：将一个月的HLL进行PFMERGE操作





### 分布式锁

#### 实现

通过redis的setNx来实现

##### 怎么支持锁的重入？

通过本地的ThreadLocal变量来实现：

1. 锁初始化的时候创建一个ThreadLocal变量
2. 第一次加锁时添加一个Lock信息（内部是一个AtomicInteger实现的计数器）
3. 后续每次加锁都进行重入检查，如果存在ThreadLocal的计数器变量，则直接增加计数器，不同加锁

##### 怎么保证加锁线程与解锁线程一致？

通过ThreadLocal变量实现，如果没有ThreadLocal变量，则表示当前线程不持有锁（或已经释放了锁）

##### 锁超时时间怎么确定？业务逻辑未处理完，但是锁超时这种场景如何处理？会造成什么问题？

锁的设置一般需要比业务逻辑执行的平均时间要长；

通过WatchDog来实现一个锁的续命：锁创建之后，同时创建一个后台线程的定时器，在超时之前对锁进行续命

如果不续命的话，会有死锁的风险；比如当前线程持有A、B 两个锁，但是A锁超时释放了，这是锁B也无法释放，只能等待超时释放；

##### 现有redis分布式锁有问题？

当前基于redis 实现的分布式锁，由于数据是保存在redis单节点上的，所以redis单节点挂了，数据可能丢失；（主从未及时同步）

##### 分布式锁：RedLock的实现

现有的redis分布式锁，由于数据是保存在单节点上，有单点故障的风险；RedLock就是基于多节点副本（多个主节点）的方式来实现的；

假设当前redis集群是N个节点，锁的有效时间为t

1. 获取当前客户端时间

2. 依次尝试对N个redis节点加锁，对每个节点加锁过程设置超时时间（需要远远小于锁的超时时间），防止由于某个节点长时间未获取到锁阻塞对其它节点获取锁，同时也防止出现获取锁的时间超过锁的有效时间；

3. 对所有端加锁完毕后，客户端再计算获取锁过程的耗时 t2，锁获取成功的判断条件如下：

   a. t2 < t，即锁还存在有效时间

   b. 超过半数（N/2 + 1）的节点获取锁成功

4. 如果锁获取成功，则重新设置锁的有效时间为t2 -t 

5. 如果锁获取失败，则对获取到锁的节点释放锁；



##### ZK实现分布式锁与redis实现的区别和优劣？





### 场景应用

#### 场景一：微信朋友圈设计，点赞，评论功能实现，拉黑呢，redis数据没了怎么办



#### 场景二：怎么统计一亿用户的日活，hyperloglog有什么缺点，bitmap不行么

可以使用HLL统计或者BitMap；

HLL缺点：HLL是基于统计的数据，数据不够精确；

BitMap可以使用，但是占用空间较大，需要12MB的数据，且BitMap调用Bitcount统计较慢；



#### 场景三：如何评估系统需要的缓存容量？

二八原则，20%的缓存容量可以承载80%的请求，正常容量设置为所有数据量的20%左右；

对于热点数据的容量可以另外评估，且不设置过期时间；







# Redis问题画像

## 性能问题

### redis中的慢操作（耗时操作）

1. 



## 主从问题



## 集群问题

### 脑裂问题



### Codis与redis cluster



## 缓存问题

### 缓存击穿

#### 定义

热点数据突然失效

#### 解决方案

热点数据**永不过期**



### 缓存穿透

#### 定义

大量访问不存在的数据

#### 解决方案

使用**布隆过滤器**（有一定误判）

#### 布隆过滤器

**特点：**

1. 如果布隆过滤器判断一个元素存在，那么这个元素**可能存在**
2. 如果布隆过滤器判断一个元素不存在，那么这个元素**一定不存在**

**布隆过滤器原理：**

1. 创建一个位图（BitMap）
2. 对Key进行多次hash运算（多个哈希函数），得到多个散列值
3. 将第二步得到的散列值作为下标，将BitMap中对应的位设置为1
4. 使用时，就判断BitMap中对应散列值的位是否都是1，如果都是1，则表明数据存在，否则数据不存在



#### 布隆过滤器的问题

1. 有一定的误判（误判率比较低）
2. 不支持删除数据

#### 布隆过滤器的解决方案

1. 降低误判：使用多个hash函数，算多个散列值
2. 支持数据删除：使用带计数功能的布隆过滤器（不再只记录0和1，多个元素就向上累加）



总结：BitMap、HyperLogLog、BloomFilter（本质是BitMap） 都提供了快速去重的功能，对于精确度、存储空间、性能都有不同的特点



### 缓存雪崩

#### 定义

大量缓存在同一时间过期

#### 解决方案

设置随机过期时间



### 缓存一致性

本质是缓存读写的问题：先写缓存还是先写DB？

参考文章：https://zhuanlan.zhihu.com/p/150864858

结论：并发场景下还是应该**先写DB再删除缓存**；

#### 为什么是删除缓存而不是更新缓存？

因为并发场景下，直接更新缓存会**造成BD数据与缓存数据的不一致**；因为更新DB与更新缓存不在一个事务里面，可能A线程先更新的数据库，但却后更新缓存，导致数据库数据与缓存数据不一致；

#### 为什么先写DB再删除缓存？

因为先写缓存会造成DB与缓存的不一致



### 场景一：缓存代码如何写？（读写缓存）

#### 缓存读

**需要考虑的问题：**

1. 缓存击穿问题；

**过程**

1. 先查缓存数据，如果存在返回
2. 如果缓存未命中，则查布隆过滤器，如果不存在，直接返回
3. 布隆过滤器存在，则访问DB查询（这步可以做限流）
4. 查询DB数据后，写入缓存（缓存可以由异步的线程，监听数据库binlog写入）



#### 缓存写

需要考虑缓存与数据库一致性的问题

**过程**

1. 先写DB
2. 将缓存失效



## 内存问题





# 相关链接

### 面试问题

https://zhuanlan.zhihu.com/p/368211750

https://mp.weixin.qq.com/s/4hNqqmub7ESrMqgTIn6coA

### 推荐书籍

《Redis使用手册》

《Redis设计与实现》

《Redis开发与运维》
